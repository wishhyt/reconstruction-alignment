wandb:
  entity: null
#  run_id: askkz9i2
  resume: 'auto'

experiment:
    project: "RecA"
    name: "show-o-reca-clip"
    output_dir: "show-o-reca-clip"
    max_train_examples_t2i: 20000000
    max_train_examples_mmu: 40000000
    save_every: 1000
    eval_every: 5000
    generate_every: 1000
    log_every: 5
    log_grad_norm_every: 500
    resume_from_checkpoint: 'latest'

model:
    vq_model:
        type: "magvitv2"
        vq_model_name: "showlab/magvitv2"

    showo:
        load_from_showo: True
        pretrained_model_path: "showlab/show-o-w-clip-vit-512x512"
        w_clip_vit: True
        vocab_size: 58498
        llm_vocab_size: 50295
        llm_model_path: 'microsoft/phi-1_5'
        codebook_size: 8192
        num_vq_tokens: 1024
        num_new_special_tokens: 10  # <|soi|> <|eoi|> <|sov|> <|eov|> <|t2i|> <|mmu|> <|t2v|> <|v2v|> <|lvg|> <|pad|>

    gradient_checkpointing: True

dataset:
    gen_type: "t2i"
    recon_type: "mid"
    und_type: "llava_tuning"
    combined_loader_mode: "min_size"
    add_system_prompt: False
    params:
        train_t2i_shards_path_or_url: [ "/home/jixie/JourneyDB/blobs/{000..009}.tgz" ]
        # train_mmu_shards_path_or_url: [ "/home/jixie/cc12m-train-{0000..0099}.tar",
                                        # "/mnt/bn/vgfm/laion5b/laion-aesthetics-12m-images/{00000..01209}.tar" 
                                        # ]
        # train_lm_shards_path_or_url: "/mnt/bn/vgfm2/test_mlx/xavier/data/falcon-refinedweb/data/*.parquet"
        add_caption_prompt: True
        external_caption_path: "" # "/mnt/bn/vgfm2/test_mlx/xavier/data/SAM-LLaVA-Captions10M"
        # external_journeydb_caption_path: "prompt_dict.json"
        # external_journeydb_caption_path: "/mnt/bn/vgfm2/test_mlx/xavier/code/3062/open_muse/train_journeydb_anno.json"
        # external_laion12m_caption_path: "/mnt/bn/vgfm/laion5b/laion-aesthetics-12m-captions"
        # external_cc12m_caption_path: '/mnt/bn/vgfm/cc12m/captions/'
        validation_prompts_file: "validation_prompts/text2image_prompts.txt"
        shuffle_buffer_size: 1000
        num_workers: 12
        resolution: 512
        pin_memory: True
        persistent_workers: True
        wds_seed: 42
    preprocessing:
        max_seq_length: 576 # for text tokens
#        max_seq_length: 512 # for text tokens
        resolution: 512
        center_crop: False
        random_flip: False

optimizer:
    name: adamw
    params: # default adamw params
        learning_rate: 5e-07
        scale_lr: False # scale learning rate by total batch size
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.01
        epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 1000

training:
    # use_template: True
    gradient_accumulation_steps: 5
    noise_type: "mask"
    batch_size_t2i: 1
    batch_size_lm: -1
    batch_size_mmu: 2
    batch_size_recon: 2
#    batch_size_t2i: 4
#    batch_size_lm: 1
#    batch_size_mmu: 4
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 10086
    max_train_steps: 1000000
    overfit_one_batch: False
    cond_dropout_prob: 0.1
    min_masking_rate: 0.0
    label_smoothing: 0.0
    max_grad_norm: null
    guidance_scale: 0.0
    generation_timesteps: 12
    t2i_coeff: 0.0
    lm_coeff: 0.1
    mmu_coeff: 1.0
    recon_coeff: 1.0
    # down_scale: 16
    # rand_down_scale: False
